{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4737836-127c-4d52-badb-810cf9a871c6",
   "metadata": {},
   "source": [
    "# Ragas evaluation of Conversations with Literal Doc Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a466561-393e-424a-83fb-a5a87d34020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install the local Python client, for dev purposes only. Ultimately remove from notebook.\n",
    "!pip install ../../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096a4ee5-f460-4544-b24a-b28226078ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379dd1cbd3ab41bd83399c3944c91a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_samples = {'question': [\"What's my name?\"],\n",
    " 'answer': ['Your name is John.'],\n",
    " 'contexts': [['My name is John.', 'My job is coding.']],\n",
    " 'ground_truth': ['']}\n",
    "\n",
    "# Do our Dataset integrate easily with HF's Dataset?\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas.metrics import context_relevancy\n",
    "from ragas import evaluate\n",
    "\n",
    "metrics = [context_relevancy]\n",
    "ds = Dataset.from_dict(data_samples)\n",
    "\n",
    "results = evaluate(ds, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40550040-be5b-4572-8469-a3febbe5344d",
   "metadata": {},
   "source": [
    "## Run a RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc23807-af1b-4c1c-a939-9aa277e72466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Insert of existing embedding ID: id1\n",
      "Insert of existing embedding ID: id2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from literalai import LiteralClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "literal_client = LiteralClient()\n",
    "\n",
    "# literal_client.instrument_openai()\n",
    "\n",
    "chroma_client = chromadb.PersistentClient()\n",
    "\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\"Biography\")\n",
    "collection.add(\n",
    "    documents=[\"My name is John.\", \"My job is coding.\"],\n",
    "    ids=[\"id1\", \"id2\"]\n",
    ")\n",
    "\n",
    "template_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that always answers questions. Keep it short, and if available prefer responding with code.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Answer the question based on the context below.\\nContext:\\n{{#context}}\\n{{.}}\\n{{/context}}\\nQuestion:\\n{{question}}\\nAnswer:\"\n",
    "    }\n",
    "]\n",
    "\n",
    "PROMPT_NAME = \"RAG prompt\"\n",
    "\n",
    "prompt = literal_client.api.create_prompt(name=PROMPT_NAME, template_messages=template_messages)\n",
    "\n",
    "@literal_client.step(type=\"run\", name=\"Retrieval Augmented Generation\")\n",
    "def rag(user_query: str):\n",
    "\n",
    "    with literal_client.step(type=\"run\", name=\"Retrieve\") as step:\n",
    "        step.input = { \"content\": user_query }\n",
    "        res = collection.query(query_texts=[user_query], n_results=2)\n",
    "        step.output = { \"content\": res }\n",
    "\n",
    "    with literal_client.step(type=\"run\", name=\"LLM\") as step:\n",
    "        step.input = { \"content\": res['documents'][0] }\n",
    "        messages = prompt.format({\"context\": res['documents'][0], \"question\": user_query})\n",
    "        \n",
    "        completion = openai_client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=messages,\n",
    "                )\n",
    "        step.output = { \"content\": completion.choices[0].message.content }\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "        \n",
    "def main():\n",
    "    question = \"What's my name?\"\n",
    "    with literal_client.thread(name=\"Example\") as thread:\n",
    "        literal_client.message(content=question, type=\"user_message\", name=\"User\")\n",
    "        answer = rag(question)\n",
    "        literal_client.message(content=answer, type=\"assistant_message\", name=\"My Assistant\")\n",
    "\n",
    "main()\n",
    "# Network requests by the SDK are performed asynchronously.\n",
    "# Invoke flush_and_stop() to guarantee the completion of all requests prior to the process termination.\n",
    "# WARNING: If you run a continuous server, you should not use this method.\n",
    "literal_client.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3302a129-83fb-461e-956b-019871e1c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from literalai import LiteralClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "literal_client = LiteralClient()\n",
    "\n",
    "literal_client.instrument_openai()\n",
    "\n",
    "\n",
    "def main():\n",
    "    question = \"What's my name?\"\n",
    "    with literal_client.thread(name=\"Example\") as thread:\n",
    "        literal_client.message(content=\"coudou\", type=\"assistant_message\", name=\"My Assistant\")\n",
    "\n",
    "main()\n",
    "# Network requests by the SDK are performed asynchronously.\n",
    "# Invoke flush_and_stop() to guarantee the completion of all requests prior to the process termination.\n",
    "# WARNING: If you run a continuous server, you should not use this method.\n",
    "# literal_client.flush_and_stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34389aa5-cd8b-4852-8202-b929864bc79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4eabb05d9d94313b82c1d7d697cd75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [context_relevancy]\n\u001b[1;32m     13\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(data_samples)\n\u001b[0;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate(ds, metrics\u001b[38;5;241m=\u001b[39mmetrics, is_async\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/ragas/evaluation.py:211\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/ragas/executor.py:132\u001b[0m, in \u001b[0;36mExecutor.results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m executor_job\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     executor_job\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_samples = {'question': [\"What's my name?\"],\n",
    " 'answer': ['Your name is John.'],\n",
    " 'contexts': [['My name is John.', 'My job is coding.']],\n",
    " 'ground_truth': ['']}\n",
    "\n",
    "# Do our Dataset integrate easily with HF's Dataset?\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas.metrics import context_relevancy, faithfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "metrics = [context_relevancy]\n",
    "ds = Dataset.from_dict(data_samples)\n",
    "\n",
    "results = evaluate(ds, metrics=metrics, is_async=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05484e7e-708f-43dd-8ed4-6d2460ef747e",
   "metadata": {},
   "source": [
    "## Create an empty Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d2c58a-7ed8-4af7-9a34-2a798767f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = f\"Literal documentation RAG 523\"\n",
    "\n",
    "dataset = literal_client.api.get_dataset(name=DATASET_NAME)\n",
    "if not dataset:\n",
    "    dataset = literal_client.api.create_dataset(name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7876ad2-b58d-47cf-a4af-71ace0bbb6c4",
   "metadata": {},
   "source": [
    "## Add \"Query\" Steps to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1901bf23-1501-413a-b3ab-dfea64050710",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = literal_client.api.get_threads(first=1).data\n",
    "\n",
    "query_steps = []\n",
    "for thread in threads:\n",
    "    thread_query_steps = [step for step in thread.steps if step.name == \"Retrieval Augmented Generation\"]\n",
    "    query_steps.extend(thread_query_steps)\n",
    "\n",
    "for step in query_steps:\n",
    "    dataset.add_step(step.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52608973-37bf-4c3d-bfe3-b890d7e09fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf597ef7cb54fee9526c3633a8af410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the first super bowl?</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>[The First AFL–NFL World Championship Game was...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who won the most super bowls?</td>\n",
       "      <td>The most super bowls have been won by The New ...</td>\n",
       "      <td>[The Green Bay Packers...Green Bay, Wisconsin....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question  \\\n",
       "0  When was the first super bowl?   \n",
       "1   Who won the most super bowls?   \n",
       "\n",
       "                                              answer  \\\n",
       "0       The first superbowl was held on Jan 15, 1967   \n",
       "1  The most super bowls have been won by The New ...   \n",
       "\n",
       "                                            contexts  faithfulness  \n",
       "0  [The First AFL–NFL World Championship Game was...           1.0  \n",
       "1  [The Green Bay Packers...Green Bay, Wisconsin....           0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5bcbc7a-d27f-42d8-bb79-cf3efde1c74e",
   "metadata": {},
   "source": [
    "# Prepare Ragas data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9a25e08-3e9a-4466-82ca-e598743df1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from literalai import DatasetItem\n",
    "from typing import List\n",
    "\n",
    "# For each query in our dataset, build contexts as Ragas expects them\n",
    "contexts: List[List[str]] = []\n",
    "\n",
    "items = dataset.items\n",
    "\n",
    "for item in items:\n",
    "    retrieve_step = next(step for step in item.intermediary_steps if step['name'] == \"Retrieve\")\n",
    "    # Remove unnecessary layers (content), ideally only \"expectedOutput\"\n",
    "    contexts.append(retrieve_step[\"expectedOutput\"]['content'][\"documents\"][0])\n",
    "\n",
    "# Data samples as expected by Ragas\n",
    "data_samples = {\n",
    "    'question': [item.input[\"args\"][0] for item in items],\n",
    "    'answer': [ item.expected_output[\"content\"] for item in items ],\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': [\"\"]* len(items) # No need for ground truths for: faithfulness (claims ratio), answer relevance (potential questions), context relevancy, aspect critique\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b613945-6e62-4f32-9eb9-1cecf37f1065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': [\"What's my name?\"],\n",
       " 'answer': ['Your name is John.'],\n",
       " 'contexts': [['My name is John.', 'My job is coding.']],\n",
       " 'ground_truth': ['']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "301898fa-fa1e-44cd-b466-dd2ee9390b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038a527ce39a4ae2b0b5bfc874c02806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the first super bowl?</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>[The First AFL–NFL World Championship Game was...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who won the most super bowls?</td>\n",
       "      <td>The most super bowls have been won by The New ...</td>\n",
       "      <td>[The Green Bay Packers...Green Bay, Wisconsin....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question  \\\n",
       "0  When was the first super bowl?   \n",
       "1   Who won the most super bowls?   \n",
       "\n",
       "                                              answer  \\\n",
       "0       The first superbowl was held on Jan 15, 1967   \n",
       "1  The most super bowls have been won by The New ...   \n",
       "\n",
       "                                            contexts  faithfulness  \n",
       "0  [The First AFL–NFL World Championship Game was...           1.0  \n",
       "1  [The Green Bay Packers...Green Bay, Wisconsin....           0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "from ragas.metrics import faithfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "ds = Dataset.from_dict(data_samples)\n",
    "score = evaluate(ds,metrics=[faithfulness])\n",
    "score.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda447e-4478-4a79-be74-3c36c46e29b1",
   "metadata": {},
   "source": [
    "## Evaluate with Ragas\n",
    "\n",
    "We will evaluate context relevancy which checks how relevant the retrieved contexts are to answer the user's question. \n",
    "\n",
    "The more unneeded details in the contexts, the less relevant (between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5dedab-527d-48d1-a861-32e4a9d3adda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e6b662b-7126-4a92-9703-45db0d49557c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a721d59c6424d29b849d09900816d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [context_relevancy]\n\u001b[1;32m     13\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(data_samples)\n\u001b[0;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate(ds, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/miniconda3/envs/ragas-eval/lib/python3.12/site-packages/ragas/evaluation.py:211\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[0;32m~/miniconda3/envs/ragas-eval/lib/python3.12/site-packages/ragas/executor.py:132\u001b[0m, in \u001b[0;36mExecutor.results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m executor_job\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     executor_job\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ragas-eval/lib/python3.12/threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/ragas-eval/lib/python3.12/threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_samples = {'question': [\"What's my name?\"],\n",
    " 'answer': ['Your name is John.'],\n",
    " 'contexts': [['My name is John.', 'My job is coding.']],\n",
    " 'ground_truth': ['']}\n",
    "\n",
    "# Do our Dataset integrate easily with HF's Dataset?\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas.metrics import context_relevancy\n",
    "from ragas import evaluate\n",
    "\n",
    "metrics = [context_relevancy]\n",
    "ds = Dataset.from_dict(data_samples)\n",
    "\n",
    "results = evaluate(ds, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a9d50e0-c4b6-4a08-88db-12ba49a98f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello</td>\n",
       "      <td>Hello! How can I assist you today?</td>\n",
       "      <td>[title:\"Evaluation\"_description:None_content: ...</td>\n",
       "      <td></td>\n",
       "      <td>0.003185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question                              answer  \\\n",
       "0    Hello  Hello! How can I assist you today?   \n",
       "\n",
       "                                            contexts ground_truth  \\\n",
       "0  [title:\"Evaluation\"_description:None_content: ...                \n",
       "\n",
       "   context_relevancy  \n",
       "0           0.003185  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b8c87-25c1-4a95-a593-aaa906f8a17f",
   "metadata": {},
   "source": [
    "## Create Literal experiment and log results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4d02921-65c2-4f79-9ba1-464a08e6bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from literalai import ScoreDict\n",
    "\n",
    "experiment = dataset.create_experiment(name=\"Ragas - Context relevancy In Prod (#chunks = 5)\", prompt_id=prompt.id, assertions={\"type\": \"context relevancy\"})\n",
    "\n",
    "# Log each experiment result.\n",
    "for index, row in results.iterrows():\n",
    "    scores = [  \n",
    "        { \n",
    "            \"name\": metric.name,\n",
    "            \"type\": \"AI\",\n",
    "            \"value\": row[metric.name]\n",
    "        } \n",
    "        for metric in metrics\n",
    "    ]\n",
    "\n",
    "    experiment_item = {\n",
    "        \"datasetItemId\": items[index].id,\n",
    "        \"scores\": scores,\n",
    "        \"input\": { \"content\": row[\"question\"] },\n",
    "        \"output\": { \"content\": row[\"answer\"] }\n",
    "    }\n",
    "    \n",
    "    experiment.log(experiment_item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162011d8-0bf6-4a40-b3ed-f75ee27c3449",
   "metadata": {},
   "source": [
    "## Change number of retrieved contexts and evaluate context relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8084f7-1049-4337-b612-1962b2684691",
   "metadata": {},
   "source": [
    "### Call to Chroma DB to get the necessary contexts\n",
    "\n",
    "Make parameter changes to your vector database and retrieved new contexts from the questions. \n",
    "\n",
    "Here we'll simply get the top 2 contexts instead of top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "120c1515-ad90-4fc8-a0b9-3fabb826cf8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bbf352b42249bfb7941a36985ef926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_samples = {\n",
    "    'question': [json.loads(item.input[\"content\"])[\"args\"][0] for item in items],\n",
    "    'answer': [ item.expected_output[\"content\"] for item in items ],\n",
    "    'contexts': [x[:2] for x in contexts], # Select the top 2 contexts\n",
    "    'ground_truth': [\"\"]* len(items)\n",
    "}\n",
    "\n",
    "results = evaluate(Dataset.from_dict(data_samples), metrics=metrics).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1184b-38ad-4af3-af4c-3120da43a201",
   "metadata": {},
   "source": [
    "### Log experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9301895-458b-4bec-94bc-5fbced435ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = dataset.create_experiment(name=\"Ragas - Context relevancy (#chunks = 2)\", prompt_id=prompt.id, assertions={\"type\": \"context relevancy\"})\n",
    "\n",
    "# Log each experiment result.\n",
    "for index, row in results.iterrows():\n",
    "    scores = [  \n",
    "        { \n",
    "            \"name\": metric.name,\n",
    "            \"type\": \"AI\",\n",
    "            \"value\": row[metric.name]\n",
    "        } \n",
    "        for metric in metrics\n",
    "    ]\n",
    "\n",
    "    experiment_item = {\n",
    "        \"datasetItemId\": items[index].id,\n",
    "        \"scores\": scores,\n",
    "        \"input\": { \"content\": row[\"question\"] },\n",
    "        \"output\": { \"content\": row[\"answer\"] }\n",
    "    }\n",
    "    \n",
    "    experiment.log(experiment_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68819d-3db8-4022-88a1-0e7a0e6860c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
