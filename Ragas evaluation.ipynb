{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4737836-127c-4d52-badb-810cf9a871c6",
   "metadata": {},
   "source": [
    "# Ragas evaluation of Conversations with Literal Doc Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a466561-393e-424a-83fb-a5a87d34020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install the local Python client.\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ff239-c466-4ef5-bfff-d2ebe2a16c78",
   "metadata": {},
   "source": [
    "## Setup connection to Literal\n",
    "\n",
    "Specify whether to connect to the production environment or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1de99c-2982-41b8-ab24-ff8ba4dd46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from literalai import LiteralClient\n",
    "\n",
    "env_configs = {\n",
    "    \"prod\": {\n",
    "        \"key\": \"\",\n",
    "        \"url\": \"https://cloud.getliteral.ai\"\n",
    "    },\n",
    "    \"local\": {\n",
    "        \"key\": \"my-initial-api-key\",\n",
    "        \"url\": \"http://localhost:3000\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def set_key_url(key: str, url: str):\n",
    "    os.environ[\"LITERAL_API_KEY\"] = key\n",
    "    os.environ[\"LITERAL_API_URL\"] = url\n",
    "\n",
    "set_key_url(**env_configs[\"local\"])\n",
    "\n",
    "literal_client = LiteralClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40550040-be5b-4572-8469-a3febbe5344d",
   "metadata": {},
   "source": [
    "## Create a RAG prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37fe871c-588b-4ec2-b53a-b50022a3c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that always answers questions. Keep it short, and if available prefer responding with code.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Answer the question based on the context below.\\nContext:\\n{{#context}}\\n{{.}}\\n{{/context}}\\nQuestion:\\n{{question}}\\nAnswer:\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Has to align with the prompt name your RAG application uses.\n",
    "PROMPT_NAME = \"RAG prompt\"\n",
    "\n",
    "prompt = literal_client.api.create_prompt(name=PROMPT_NAME, template_messages=template_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b49735-d78a-41d6-b766-097d0e46adce",
   "metadata": {},
   "source": [
    "## Launch RAG Literal Doc application\n",
    "\n",
    "Ask a few questions, like: \n",
    "- What is a dataset in Literal?\n",
    "- How can I add a Step to an existing Dataset in Python?\n",
    "- How would I go about removing an item from a dataset in TS?\n",
    "\n",
    "Create a new chat to trigger new threads.\n",
    "\n",
    "Then visualize the threads and steps from the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05484e7e-708f-43dd-8ed4-6d2460ef747e",
   "metadata": {},
   "source": [
    "## Create an empty Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d2c58a-7ed8-4af7-9a34-2a798767f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = literal_client.api.create_dataset(name=\"Literal documentation RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7876ad2-b58d-47cf-a4af-71ace0bbb6c4",
   "metadata": {},
   "source": [
    "## Add \"Query\" Steps to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1901bf23-1501-413a-b3ab-dfea64050710",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = literal_client.api.get_threads(first=1).data\n",
    "\n",
    "query_steps = []\n",
    "for thread in threads:\n",
    "    thread_query_steps = [step for step in thread.steps if step.name == \"Query\"]\n",
    "    query_steps.extend(thread_query_steps)\n",
    "\n",
    "for step in query_steps:\n",
    "    dataset.add_step(step.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcbc7a-d27f-42d8-bb79-cf3efde1c74e",
   "metadata": {},
   "source": [
    "# Prepare Ragas data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a25e08-3e9a-4466-82ca-e598743df1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from literalai import DatasetItem\n",
    "from typing import List\n",
    "\n",
    "# For each query in our dataset, build contexts as Ragas expects them\n",
    "contexts: List[List[str]] = []\n",
    "\n",
    "items = [DatasetItem.from_dict(item_dict) for item_dict in dataset.items]\n",
    "for item in items:\n",
    "    # Find the first \"Retrieve\" step in the intermediary steps.\n",
    "    retrieve_step = next(step for step in item.intermediary_steps if step['name'] == \"Retrieve\")\n",
    "\n",
    "    # Return all contexts in that step's output.\n",
    "    matches = json.loads(retrieve_step['expectedOutput']['content'])[\"metadatas\"][0]\n",
    "    contexts.append([match[\"text\"] for match in matches])\n",
    "\n",
    "# Data samples as expected by Ragas\n",
    "data_samples = {\n",
    "    'question': [json.loads(item.input[\"content\"])[\"args\"][0] for item in items],\n",
    "    'answer': [ item.expected_output[\"content\"] for item in items ],\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': [\"\"]* len(items) # No need for ground truths for: faithfulness (claims ratio), answer relevance (potential questions), context relevancy, aspect critique\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda447e-4478-4a79-be74-3c36c46e29b1",
   "metadata": {},
   "source": [
    "## Evaluate with Ragas\n",
    "\n",
    "We will evaluate context relevancy which checks how relevant the retrieved contexts are to answer the user's question. \n",
    "\n",
    "The more unneeded details in the contexts, the less relevant (between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e6b662b-7126-4a92-9703-45db0d49557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from ragas.metrics import context_relevancy\n",
    "from ragas import evaluate\n",
    "\n",
    "metrics = [context_relevancy]\n",
    "\n",
    "results = evaluate(Dataset.from_dict(data_samples), metrics=metrics).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a9d50e0-c4b6-4a08-88db-12ba49a98f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I add a Step to an existing Dataset in...</td>\n",
       "      <td>To add a Step to an existing Dataset in Python...</td>\n",
       "      <td>[title:Create and Populate a Dataset_descripti...</td>\n",
       "      <td></td>\n",
       "      <td>0.001984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a dataset in Literal?</td>\n",
       "      <td>A dataset in Literal is a collection of items ...</td>\n",
       "      <td>[title:Overview_description:None_content:  Lit...</td>\n",
       "      <td></td>\n",
       "      <td>0.064103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How would I go about removing an item from a d...</td>\n",
       "      <td>To remove an item from a dataset in TypeScript...</td>\n",
       "      <td>[title:Create and Populate a Dataset_descripti...</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How can I add a Step to an existing Dataset in...   \n",
       "1                      What is a dataset in Literal?   \n",
       "2  How would I go about removing an item from a d...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  To add a Step to an existing Dataset in Python...   \n",
       "1  A dataset in Literal is a collection of items ...   \n",
       "2  To remove an item from a dataset in TypeScript...   \n",
       "\n",
       "                                            contexts ground_truth  \\\n",
       "0  [title:Create and Populate a Dataset_descripti...                \n",
       "1  [title:Overview_description:None_content:  Lit...                \n",
       "2  [title:Create and Populate a Dataset_descripti...                \n",
       "\n",
       "   context_relevancy  \n",
       "0           0.001984  \n",
       "1           0.064103  \n",
       "2           0.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b8c87-25c1-4a95-a593-aaa906f8a17f",
   "metadata": {},
   "source": [
    "## Create Literal experiment and log results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4d02921-65c2-4f79-9ba1-464a08e6bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from literalai import ScoreDict\n",
    "\n",
    "experiment = dataset.create_experiment(name=\"Ragas - Context relevancy In Prod (#chunks = 5)\", prompt_id=prompt.id, assertions={\"type\": \"context relevancy\"})\n",
    "\n",
    "# Log each experiment result.\n",
    "for index, row in results.iterrows():\n",
    "    scores = [  \n",
    "        { \n",
    "            \"name\": metric.name,\n",
    "            \"type\": \"AI\",\n",
    "            \"value\": row[metric.name]\n",
    "        } \n",
    "        for metric in metrics\n",
    "    ]\n",
    "\n",
    "    experiment_item = {\n",
    "        \"datasetItemId\": items[index].id,\n",
    "        \"scores\": scores,\n",
    "        \"input\": { \"content\": row[\"question\"] },\n",
    "        \"output\": { \"content\": row[\"contexts\"] }\n",
    "    }\n",
    "    \n",
    "    experiment.log(experiment_item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162011d8-0bf6-4a40-b3ed-f75ee27c3449",
   "metadata": {},
   "source": [
    "## Change number of retrieved contexts and evaluate context relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8084f7-1049-4337-b612-1962b2684691",
   "metadata": {},
   "source": [
    "### Call to Chroma DB to get the necessary contexts\n",
    "\n",
    "Make parameter changes to your vector database and retrieved new contexts from the questions. \n",
    "\n",
    "Here we'll simply get the top 2 contexts instead of top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "120c1515-ad90-4fc8-a0b9-3fabb826cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "data_samples = {\n",
    "    'question': [json.loads(item.input[\"content\"])[\"args\"][0] for item in items],\n",
    "    'answer': [ item.expected_output[\"content\"] for item in items ],\n",
    "    'contexts': [x[:2] for x in contexts], # Select the top 2 contexts\n",
    "    'ground_truth': [\"\"]* len(items)\n",
    "}\n",
    "\n",
    "results = evaluate(Dataset.from_dict(data_samples), metrics=metrics).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1184b-38ad-4af3-af4c-3120da43a201",
   "metadata": {},
   "source": [
    "### Log experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9301895-458b-4bec-94bc-5fbced435ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = dataset.create_experiment(name=\"Ragas - Context relevancy (#chunks = 2)\", prompt_id=prompt.id, assertions={\"type\": \"context relevancy\"})\n",
    "\n",
    "# Log each experiment result.\n",
    "for index, row in results.iterrows():\n",
    "    scores = [  \n",
    "        { \n",
    "            \"name\": metric.name,\n",
    "            \"type\": \"AI\",\n",
    "            \"value\": row[metric.name]\n",
    "        } \n",
    "        for metric in metrics\n",
    "    ]\n",
    "\n",
    "    experiment_item = {\n",
    "        \"datasetItemId\": items[index].id,\n",
    "        \"scores\": scores,\n",
    "        \"input\": { \"content\": row[\"question\"] },\n",
    "        \"output\": { \"content\": row[\"contexts\"] }\n",
    "    }\n",
    "    \n",
    "    experiment.log(experiment_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68819d-3db8-4022-88a1-0e7a0e6860c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
