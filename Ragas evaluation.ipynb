{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4737836-127c-4d52-badb-810cf9a871c6",
   "metadata": {},
   "source": [
    "# Ragas evaluation of Conversations with Literal Doc Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a466561-393e-424a-83fb-a5a87d34020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install the local Python client.\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ff239-c466-4ef5-bfff-d2ebe2a16c78",
   "metadata": {},
   "source": [
    "## Setup connection to Literal\n",
    "\n",
    "Specify whether to connect to the production environment or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1de99c-2982-41b8-ab24-ff8ba4dd46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from literalai import LiteralClient\n",
    "\n",
    "env_configs = {\n",
    "    \"prod\": {\n",
    "        \"key\": \"\",\n",
    "        \"url\": \"https://cloud.getliteral.ai\"\n",
    "    },\n",
    "    \"local\": {\n",
    "        \"key\": \"my-initial-api-key\",\n",
    "        \"url\": \"http://localhost:3000\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def set_key_url(key: str, url: str):\n",
    "    os.environ[\"LITERAL_API_KEY\"] = key\n",
    "    os.environ[\"LITERAL_API_URL\"] = url\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "set_key_url(**env_configs[\"local\"])\n",
    "\n",
    "literal_client = LiteralClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40550040-be5b-4572-8469-a3febbe5344d",
   "metadata": {},
   "source": [
    "## Create a RAG prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37fe871c-588b-4ec2-b53a-b50022a3c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that always answers questions. Keep it short, and if available prefer responding with code.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Answer the question based on the context below.\\nContext:\\n{{#context}}\\n{{.}}\\n{{/context}}\\nQuestion:\\n{{question}}\\nAnswer:\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Has to align with the prompt name your RAG application uses.\n",
    "PROMPT_NAME = \"RAG prompt\"\n",
    "\n",
    "prompt = literal_client.api.create_prompt(name=PROMPT_NAME, template_messages=template_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b49735-d78a-41d6-b766-097d0e46adce",
   "metadata": {},
   "source": [
    "## Launch RAG Literal Doc application\n",
    "\n",
    "Ask a few questions, like: \n",
    "- What is a dataset in Literal?\n",
    "- How can I add a Step to an existing Dataset in Python?\n",
    "- How would I go about removing an item from a dataset in TS?\n",
    "\n",
    "Create a new chat to trigger new threads.\n",
    "\n",
    "Then visualize the threads and steps from the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05484e7e-708f-43dd-8ed4-6d2460ef747e",
   "metadata": {},
   "source": [
    "## Create an empty Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d2c58a-7ed8-4af7-9a34-2a798767f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"Literal documentation RAG\"\n",
    "\n",
    "dataset = literal_client.api.get_dataset(name=DATASET_NAME)\n",
    "if not dataset:\n",
    "    dataset = literal_client.api.create_dataset(name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7876ad2-b58d-47cf-a4af-71ace0bbb6c4",
   "metadata": {},
   "source": [
    "## Add \"Query\" Steps to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1901bf23-1501-413a-b3ab-dfea64050710",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = literal_client.api.get_threads(first=1).data\n",
    "\n",
    "query_steps = []\n",
    "for thread in threads:\n",
    "    thread_query_steps = [step for step in thread.steps if step.name == \"Query\"]\n",
    "    query_steps.extend(thread_query_steps)\n",
    "\n",
    "for step in query_steps:\n",
    "    dataset.add_step(step.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcbc7a-d27f-42d8-bb79-cf3efde1c74e",
   "metadata": {},
   "source": [
    "# Prepare Ragas data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a25e08-3e9a-4466-82ca-e598743df1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dataset_id': 'dataset_1', 'text': 'title:\"Evaluation\"_description:None_content:  Contact Us [here](https://cal.com/dan-constantini/15min) to know more about how we evaluate LLM apps.  '}, {'dataset_id': 'dataset_1', 'text': 'title:Projects & Roles_description:None_content:  With the ability to create projects on Literal, you can compartmentalize different parts of your LLM Product.  ### Creating & inviting members a Project  To start a new project, navigate to the bottom part and create a new project.  Once a project is created, you can invite others to collaborate:  1. Go to the project settings. 2. Invite members by entering their email addresses. 3. Assign roles to each member:    - **Admin**: Full access to the project.    - **User**: Read-only access to the project.  <Frame caption=\"Invite a collaborator\">   <img src=\"/images/members.jpg\" alt=\"Invite a collaborator\" /> </Frame>  Contact us to create more fine-grained roles.  ### Managing API Keys  You can then navigate to the \"Settings\" tab in your project, access the \"API Keys\" section, generate a new API key, and use it in your application.  ### Project-level settings  Project-level settings in Literal:  - **LLM Providers**: Set up credentials for LLM providers like Groq, Anthropic, OpenAI, and Mistral. - **Tags**: Organize project threads and steps with tags. Add or remove as needed. - **Score Templates**: Craft score templates to assess LLM application performance.  See documentation for setup details or contact support for help.  <Frame caption=\"Project-level settings\">   <img src=\"/images/settings-panel.jpg\" alt=\"Project-level settings\"/> </Frame>  '}, {'dataset_id': 'dataset_1', 'text': 'title:Quick Start_description:None_content:  Once you have your project setup and the Literal SDK installed, the fastest way to start logging your data is to **leverage our integrations**.  ## Integrations  Literal comes with a set of integrations with popular libraries of the GenAI field. Let us know if you want to see a new integration!   <CardGroup cols={2}>  <Card   title=\"OpenAI\"   icon=\"circle\"   color=\"#ea5a0c\"   href=\"/integrations/openai\"> >   Learn how to monitor your OpenAI completions with Literal. </Card>    <Card   title=\"Langchain\"   icon=\"circle\"   color=\"#0285c7\"   href=\"/integrations/langchain\">     Learn how to use monitor any Langchain agent with Literal.   </Card>  <Card   title=\"OpenAI Assistant\"   icon=\"circle\"   color=\"#dc2626\"   href=\"/integrations/openai-assistant\"> >   Learn how to monitor your OpenAI Assistants with Literal. </Card>    <Card   title=\"Chainlit\"   icon=\"circle\"   color=\"#F80061\"   href=\"/integrations/chainlit\">     Learn how to use monitor your Chainlit application with Literal.   </Card>  </CardGroup>  ## Next up  <Card title=\"Literal\\'s Concepts\" icon=\"graduation-cap\" href=\"/concepts\">   Learn more about Literal\\'s Concepts and Entities </Card>'}, {'dataset_id': 'dataset_1', 'text': 'title:\"Prompt\"_description:None_content:  You can view your prompts in the Prompt Management page. This overview shows the prompts that you have created, manage different versions of these prompts and lets you create and test prompt templates in collaboration with your teammates. In the [Literal Prompt Playground](#prompt-playground), which is connected to prompts and threads, you can debug prompts in-context, draft and iterates on prompts. Once you are happy with your prompt, you can save it and use it in your code.  Prompts are managed in the Literal platform, which has several benefits: - Non technical people can draft and iterate on prompts. - You can deploy a new version of your prompt without deploying your code. - You can track which prompt version for a specific generation. - You can compare prompt versions to see which one performs better. - You can compare and test multiple LLM providers. - You can collaborate with the team. - You can do online evaluation [coming soon].   ## Versioning On the `Prompts` dashboard, you can see which prompts are present and running (`Champion Version`). By clicking on a prompt, you get to a page where you can see which versions of that prompt are saved. By clicking on a version number, you enter the [`Playground`](#prompt-playground), where you can change this prompt, or promote this prompt template to the `Champion Version`.   ## Prompt Playground  The playground is an environment in which you can create, test, and debug prompts. You can access the playground via 1. **The main menu**. This allows you to create, test and save prompts from scratch. 2. **Prompts**. This allows you to view, edit and test your prompt template and LLM settings and manage prompt versioning. 3. **LLM Generations**. Via Generations you can access the playground with an individual generation from a real chat conversation. This allows you to debug a single generation of the LLM in context.  If you start from scratch in the Prompt Playground, you can open example templates, and iterate from there.  <Frame caption=\"Playground Prompt Template\">   <img src=\"/images/playground-prompt-template.png\" alt=\"An example of a prompt template in the playground.\" /> </Frame>  ### 1. Design prompt templates  On the left hand side of the view you can create prompt templates (1).  * In a `system message` you give instructions about the role of the assistant. For example, you can say that the assistant should provide concise answers.  * In `User`, you give a format of the user query to the LLM. In this example, two variables are used. First, context documents are given, in a for-loop. The content of the variables can be defined below, but are usually defined in the code of the application. Then, the question that the user entered in the chatbot window is given. * `Assistant` messages are responses by the LLM.  * `Tool` messages are responses by the tool or function.  You can also pass functions to the prompt template. They are defined in the code or in `Tools`. The LLM model can detect when a specific function needs to be called based on the user\\'s input.   #### Formatting The text and code in the prompt template is formatted according to the [the Mustache prompt templating format](https://scalate.github.io/scalate/documentation/mustache.html#Syntax). Tags are surrounded by \"mustaches\" or double curly brackets. With tags and mustaches, variables, sections, loops and functions can be declared. A variable is written like `{{variable}}`, and an if statement and for-loop is written like the following. If `x` is a boolean value, the section tag acts like an `if` conditional. When `x` is an array, this acts like a `for each` loop.   ```\\n# if statement / for loop using Mustache templating format\\n{{#x}}\\n{{.}}\\n{{/x}}\\n```  ### 2. LLM settings  On the right side of the screen (2), you can set an LLM provider (for example OpenAI or Anthropic). You can change the following settings:  * **Model.** Which model of the provider to use. * **Temperature.** Control randomness. A lower temperature results in less random generations. As the temperature approaches zero, the model will become deterministic and repetitive.  * **Maximum Length.** Maximum length of tokens to generate. Requests can use op to 32,000 tokens, shared between prompt and completion. The exact limit varies per model. * **Stop Sequences.** Use up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. * **Top P** or Nucleas Sampling. Controlling diversity by how many possible words to consider. A high Top P will look at more possible words, even the less likely ones, which makes the generated text more diverse. * **Frequency Penalty.** How much to penalize new tokes based on their existing frequency in the text so far. Decreases the model\\'s likelihood to repeat the sam line verbatim. * **Presence Penalty.** How much to penalize new tokens based on whether they appear in the text so far. Increases the model\\'s likelihood to talk about new topics.  ### 3. Try out & in-context debugging.  In the center of the screen (3), you can try out the settings and prompt template. Make sure you have an API key of the selected provided set (at the top right key icon). Then, you can send messages to the LLM using this template and settings. You can change things, and try again.   If you accessed the playground via an LLM call, you get the conversation context which you can use to try out different settings.   You can also use multimodality here. For example, you can upload an image in the chat.    ### 4. Save  Once you are happy with the template and settings, you can save the setup (4).   When you save a new template, a screen will pop up with a diff comparison. In this changelog, like using Git, you can see what you have changed, before you save these changes.   <Frame caption=\"Playground Diff Comparison\">   <img src=\"/images/playground-diff.png\" alt=\"Diff comparison when you save a new prompt template.\"/> </Frame>   ### 5. Promote  If you want to upgrade the version you opened in the Playground to the current version your application is using, you need to `Promote` the prompt template and settings. You do this by selecting the correct version (5), and clicking on the `Promote` button (6).   ## Prompt API  Via the Python and TypeScript SDK, you can acces the prompt API. With the API you can get the prompt to use in your code and add variable values in a conversation flow. You cannot draft and save prompt templates via the API, this should be done via the Literal Prompt Playground.  ### Get a Prompt  Getting a prompt is the first step to use it in your code. You can get a prompt by its name.  <CodeGroup> ```python Python\\nimport os\\nfrom literalai import LiteralClient\\n\\nclient = LiteralClient(api_key=os.getenv(\"LITERAL_API_KEY\"))\\n# This will fetch the champion version, you can also pass a specific version\\nprompt = client.api.get_prompt(name=\"RAG prompt\")\\n```  ```typescript TypeScript\\nimport { LiteralClient } from \\'@literalai/client\\';\\n\\nconst client = new LiteralClient(process.env[\\'LITERAL_API_KEY\\']);\\n\\nconst promptName = \\'RAG prompt\\';\\n// This will fetch the champion version, you can also pass a specific version\\nconst prompt = await client.api.getPrompt(promptName);\\n```  </CodeGroup>  ### Format a Prompt  Once you got your prompt, you can format it to get messages in the OpenAI format.  Combining prompts with integrations (like the [OpenAI integration](/integrations/openai)) allows you to log the generations and to track which prompt versions were used to generate them.  You format messages like this:  <CodeGroup> ```python Python\\nimport os\\nimport asyncio\\nfrom literalai import LiteralClient\\nfrom openai import AsyncOpenAI\\n\\nopenai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\\n\\nliteral_client = LiteralClient(api_key=os.getenv(\"LITERAL_API_KEY\"))\\n# Optionally instrument the openai client to log generations\\nliteral_client.instrument_openai()\\n\\nasync def main():\\n    prompt = literal_client.api.get_prompt(name=\"RAG prompt\")\\n\\n    # Optionally pass variables to the prompt\\n    query = \"What is the difference between apples and pears?\"\\n    variables = {\"question\": query}\\n    messages = prompt.format(variables)\\n\\n    # Optionally pass settings\\n    settings = {\"temperature\": 0, \"stream\": True, \"model\": \"gpt-4-turbo-preview\"}\\n\\n    stream = await openai_client.chat.completions.create(\\n        messages=messages,\\n        # Optionally pass the tools defined in the prompt\\n        tools=prompt.tools,\\n        # Pass the settings defined in the prompt\\n        **settings,\\n        stream=True\\n    )\\n\\n    async for chunk in stream:\\n        print(chunk)\\n\\nloop = asyncio.get_event_loop()\\nloop.run_until_complete(main())\\n```  ```typescript TypeScript\\nimport { LiteralClient } from \\'@literalai/client\\';\\nimport OpenAI from \\'openai\\';\\n\\nconst literalClient = new LiteralClient(process.env[\\'LITERAL_API_KEY\\']);\\n\\nconst openai = new OpenAI({\\n  apiKey: process.env[\\'OPENAI_API_KEY\\'],\\n});\\n\\nasync function main() {\\n  const prompt = await literalClient.api.getPrompt(\\'RAG prompt\\');\\n\\n  // Optionally pass variables to the prompt\\n  const variables = { query: \\'What is the difference between apples and pears?\\' };\\n\\n  const messages = prompt.format(variables);\\n\\n  const result = await openai.chat.completions.create({\\n    ...prompt.settings,\\n    tools: prompt.tools,\\n    messages: messages,\\n    stream: true\\n  });\\n\\n  await client.instrumentation.openai(result);\\n}\\n\\nmain();\\n\\n```  </CodeGroup>  ### Langchain Chat Template  Since Langchain has a different format for prompts, you can convert a Literal prompt to a Langchain Chat Template.  You can combine the prompt with the [Langchain integration](/integrations/langchain) to log the generations and to track which prompt versions were used to generate them.  <CodeGroup> ```python Python\\nimport os\\nimport asyncio\\nfrom literalai import LiteralClient\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.schema.runnable.config import RunnableConfig\\nfrom langchain.schema import StrOutputParser\\n\\nliteral_client = LiteralClient(api_key=os.getenv(\"LITERAL_API_KEY\"))\\n\\ndef main():\\n    prompt = literal_client.api.get_prompt(name=\"RAG prompt\")\\n    lc_prompt = prompt.to_langchain_chat_prompt_template()\\n\\n    model = ChatOpenAI(streaming=True)\\n    runnable = lc_prompt | model | StrOutputParser()\\n    \\n    cb = literal_client.langchain_callback()\\n    variables = {\"foo\": \"bar\"}\\n\\n    res = runnable.invoke(\\n        variables, \\n        config=RunnableConfig(callbacks=[cb], run_name=\"Test run\")\\n        )\\n\\nmain()\\n```  ```typescript TypeScript\\nimport { LiteralClient } from \\'@literalai/client\\';\\n\\nimport { StringOutputParser } from \\'@langchain/core/output_parsers\\';\\nimport { ChatOpenAI } from \\'@langchain/openai\\';\\n\\nconst literalClient = new LiteralClient(process.env[\\'LITERAL_API_KEY\\']);\\n\\nasync function main() {\\n  const prompt = await literalClient.api.getPrompt(\\'RAG prompt\\');\\n\\n  const chatPrompt = prompt.toLangchainChatPromptTemplate();\\n\\n  const model = new ChatOpenAI({});\\n  const outputParser = new StringOutputParser();\\n\\n  const chain = chatPrompt.pipe(model).pipe(outputParser);\\n\\n  const variables = { foo: \\'bar\\' };\\n\\n  const response = await chain.invoke(\\n    variables,\\n    {\\n      runName: \\'Test run\\',\\n      callbacks: [client.instrumentation.langchain.literalCallback()]\\n    }\\n  );\\n}\\n\\nmain();\\n\\n```  </CodeGroup>'}, {'dataset_id': 'dataset_1', 'text': 'title:User_description:None_content:  In Literal, the concept of a user is designed to seamlessly integrate with your application, allowing developers to create and manage user profiles that mirror their own application\\'s user base.  It\\'s important to note that the concept of User only exists at the Thread level, not at the Step level.   <Frame caption=\"Filtering Threads by User\">        <img     src=\"/images/user-filter.png\"     alt=\"The user filter on threads\"   /> </Frame>   ## Create a User  <CodeGroup> ```python Python\\nimport os\\nfrom literalai import LiteralClient\\nliteral_client = LiteralClient(api_key=os.getenv(\"LITERAL_API_KEY\"))\\n\\n@literal_client.step()\\ndef my_step(input_message):\\n    # some code, llm call, tool call, etc.\\n    answer = \"answer\"\\n    return answer\\n\\ndef run(input_message):\\n    with literal_client.thread() as thread:\\n        literal_client.message(content=input_message, type=\"user_message\", name=\"User\")\\n        user = literal_client.api.create_user(identifier=\"John Doe\")\\n        # user = await literal_client.api.get_user(identifier=\"John Doe\")\\n        thread.user = user\\n        answer = my_step(input_message)\\n        literal_client.message(content=answer, type=\"assistant_message\", name=\"Assistant\")\\n    return answer\\n```  ```typescript TypeScript\\nimport { LiteralClient, Thread } from \"@literalai/client\";\\n\\nconst client = new LiteralClient(process.env[\"LITERAL_API_KEY\"]);\\n\\n// The Assistant could have intermediary steps\\nasync function myAssistant(thread: Thread, query: string) {\\n  const run = thread.step({\\n    type: \"run\",\\n    name: \"My Assistant Run\",\\n    input: { content: query },\\n  });\\n\\n  // Implement your assistant logic here\\n  await new Promise((r) => setTimeout(r, 1000));\\n  const response = { content: \"My assistant response\" };\\n\\n  run.output = response;\\n  await run.send();\\n\\n  return response;\\n}\\n\\nasync function main() {\\n  const participantId = await client.api.getOrCreateUser(\"John Doe\");\\n\\n  // You can also continue a thread by passing the thread id\\n  const thread = await client\\n    .thread({ name: \"Thread Example\", participantId: participantId })\\n    .upsert();\\n\\n  console.log(thread.id);\\n\\n  const userQuery = \"Hello World\";\\n  await thread\\n    .step({\\n      type: \"user_message\",\\n      name: \"User\",\\n      output: { content: userQuery },\\n    })\\n    .send();\\n\\n  const assistantResponse = await myAssistant(thread, userQuery);\\n  await thread\\n    .step({\\n      type: \"assistant_message\",\\n      name: \"Assistant\",\\n      output: assistantResponse,\\n    })\\n    .send();\\n\\n  const followUpQuery = \"Follow up!\";\\n  await thread\\n    .step({\\n      type: \"user_message\",\\n      name: \"User\",\\n      output: { content: followUpQuery },\\n    })\\n    .send();\\n\\n  const followUpResponse = await myAssistant(thread, followUpQuery);\\n  await thread\\n    .step({\\n      type: \"assistant_message\",\\n      name: \"Assistant\",\\n      output: followUpResponse,\\n    })\\n    .send();\\n}\\n\\nmain()\\n  .then(() => process.exit(0))\\n  .catch((error) => console.error(error));\\n```  </CodeGroup>'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from literalai import DatasetItem\n",
    "from typing import List\n",
    "\n",
    "# For each query in our dataset, build contexts as Ragas expects them\n",
    "contexts: List[List[str]] = []\n",
    "\n",
    "items = [DatasetItem.from_dict(item_dict) for item_dict in dataset.items]\n",
    "for item in items:\n",
    "    # Find the first \"Retrieve\" step in the intermediary steps.\n",
    "    retrieve_step = next(step for step in item.intermediary_steps if step['name'] == \"Retrieve\")\n",
    "\n",
    "    # Return all contexts in that step's output.\n",
    "    matches = json.loads(retrieve_step['expectedOutput']['content'])[\"metadatas\"][0]\n",
    "    contexts.append([match[\"text\"] for match in matches])\n",
    "\n",
    "# Data samples as expected by Ragas\n",
    "data_samples = {\n",
    "    'question': [json.loads(item.input[\"content\"])[\"args\"][0] for item in items],\n",
    "    'answer': [ item.expected_output[\"content\"] for item in items ],\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': [\"\"]* len(items) # No need for ground truths for: faithfulness (claims ratio), answer relevance (potential questions), context relevancy, aspect critique\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda447e-4478-4a79-be74-3c36c46e29b1",
   "metadata": {},
   "source": [
    "## Evaluate with Ragas\n",
    "\n",
    "We will evaluate context relevancy which checks how relevant the retrieved contexts are to answer the user's question. \n",
    "\n",
    "The more unneeded details in the contexts, the less relevant (between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6b662b-7126-4a92-9703-45db0d49557c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ce08700f164014a1df3a275b90c4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from ragas.metrics import context_relevancy\n",
    "from ragas import evaluate\n",
    "\n",
    "metrics = [context_relevancy]\n",
    "\n",
    "results = evaluate(Dataset.from_dict(data_samples), metrics=metrics).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a9d50e0-c4b6-4a08-88db-12ba49a98f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello</td>\n",
       "      <td>Hello! How can I assist you today?</td>\n",
       "      <td>[title:\"Evaluation\"_description:None_content: ...</td>\n",
       "      <td></td>\n",
       "      <td>0.003185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question                              answer  \\\n",
       "0    Hello  Hello! How can I assist you today?   \n",
       "\n",
       "                                            contexts ground_truth  \\\n",
       "0  [title:\"Evaluation\"_description:None_content: ...                \n",
       "\n",
       "   context_relevancy  \n",
       "0           0.003185  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b8c87-25c1-4a95-a593-aaa906f8a17f",
   "metadata": {},
   "source": [
    "## Create Literal experiment and log results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4d02921-65c2-4f79-9ba1-464a08e6bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from literalai import ScoreDict\n",
    "\n",
    "experiment = dataset.create_experiment(name=\"Ragas - Context relevancy In Prod (#chunks = 5)\", prompt_id=prompt.id, assertions={\"type\": \"context relevancy\"})\n",
    "\n",
    "# Log each experiment result.\n",
    "for index, row in results.iterrows():\n",
    "    scores = [  \n",
    "        { \n",
    "            \"name\": metric.name,\n",
    "            \"type\": \"AI\",\n",
    "            \"value\": row[metric.name]\n",
    "        } \n",
    "        for metric in metrics\n",
    "    ]\n",
    "\n",
    "    experiment_item = {\n",
    "        \"datasetItemId\": items[index].id,\n",
    "        \"scores\": scores,\n",
    "        \"input\": { \"content\": row[\"question\"] },\n",
    "        \"output\": { \"content\": row[\"answer\"] }\n",
    "    }\n",
    "    \n",
    "    experiment.log(experiment_item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162011d8-0bf6-4a40-b3ed-f75ee27c3449",
   "metadata": {},
   "source": [
    "## Change number of retrieved contexts and evaluate context relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8084f7-1049-4337-b612-1962b2684691",
   "metadata": {},
   "source": [
    "### Call to Chroma DB to get the necessary contexts\n",
    "\n",
    "Make parameter changes to your vector database and retrieved new contexts from the questions. \n",
    "\n",
    "Here we'll simply get the top 2 contexts instead of top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "120c1515-ad90-4fc8-a0b9-3fabb826cf8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bbf352b42249bfb7941a36985ef926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_samples = {\n",
    "    'question': [json.loads(item.input[\"content\"])[\"args\"][0] for item in items],\n",
    "    'answer': [ item.expected_output[\"content\"] for item in items ],\n",
    "    'contexts': [x[:2] for x in contexts], # Select the top 2 contexts\n",
    "    'ground_truth': [\"\"]* len(items)\n",
    "}\n",
    "\n",
    "results = evaluate(Dataset.from_dict(data_samples), metrics=metrics).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1184b-38ad-4af3-af4c-3120da43a201",
   "metadata": {},
   "source": [
    "### Log experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9301895-458b-4bec-94bc-5fbced435ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = dataset.create_experiment(name=\"Ragas - Context relevancy (#chunks = 2)\", prompt_id=prompt.id, assertions={\"type\": \"context relevancy\"})\n",
    "\n",
    "# Log each experiment result.\n",
    "for index, row in results.iterrows():\n",
    "    scores = [  \n",
    "        { \n",
    "            \"name\": metric.name,\n",
    "            \"type\": \"AI\",\n",
    "            \"value\": row[metric.name]\n",
    "        } \n",
    "        for metric in metrics\n",
    "    ]\n",
    "\n",
    "    experiment_item = {\n",
    "        \"datasetItemId\": items[index].id,\n",
    "        \"scores\": scores,\n",
    "        \"input\": { \"content\": row[\"question\"] },\n",
    "        \"output\": { \"content\": row[\"answer\"] }\n",
    "    }\n",
    "    \n",
    "    experiment.log(experiment_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68819d-3db8-4022-88a1-0e7a0e6860c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
